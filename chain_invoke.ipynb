{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5056/2055349720.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/hungcq/study/AI_Chatbot/venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings, vector store, and model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"dangvantuan/vietnamese-embedding\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "setting = Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=True,\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory='.DB_llama3.1',\n",
    "    embedding_function=embeddings,\n",
    "    client_settings=setting,\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_markdown_line(string: str) -> str:\n",
    "    return f\"\\n{string}\\n\".replace(\"\\n\", \"\"\"\n",
    "\"\"\")\n",
    "\n",
    "def process_questions_from_json(json_path: str, output_md_file: str):\n",
    "    try:\n",
    "        # Step 1: Load questions from JSON file\n",
    "        with open(json_path, \"r\") as f:\n",
    "            questions_list = json.load(f)\n",
    "\n",
    "        # Step 2: Initialize a variable to store previous question's result\n",
    "        previous_response_content = \"\"\n",
    "\n",
    "        # Step 3: Loop through each question in the list\n",
    "        for question_data in questions_list:\n",
    "            question = question_data.get(\"question\")\n",
    "\n",
    "            # Step 4: Retrieve the documents\n",
    "            retrieved_docs = retriever.get_relevant_documents(question)\n",
    "            context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "            # If there's a previous question, add its result to the context\n",
    "            if question_data.get(\"is_next_question\") is True:\n",
    "                context += format_markdown_line(f\"Previous Response: {previous_response_content}\")\n",
    "\n",
    "            # Step 5: Generate the final prompt\n",
    "            prompt_template = PromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                <s> [INST] You are a helpful assistant, answer questions about ingested news documents. \n",
    "                Use only the context provided, do not use any information outside of this context. \n",
    "                If you don't know, just say that you don't know.[/INST] </s> \n",
    "                [INST] Question: {question} \n",
    "                Context: {context} \n",
    "                Answer: [/INST]\n",
    "                \"\"\"\n",
    "            )\n",
    "            prompt = prompt_template.format(question=question, context=context)\n",
    "\n",
    "            # Step 6: Send the prompt to the model\n",
    "            response = model.invoke(prompt)\n",
    "\n",
    "            # Extract model's response content and retrieved document content\n",
    "            model_response_content = response.content\n",
    "            retrieved_docs_content = \"\".join([format_markdown_line(f\"* {doc.page_content}\") for doc in retrieved_docs])\n",
    "\n",
    "            # Step 7: Append results to markdown file\n",
    "            try:\n",
    "                with open(output_md_file, \"a\") as md_file:\n",
    "                    md_file.write(format_markdown_line(f\"## Question: {question}\"))\n",
    "                    md_file.write(\n",
    "                        format_markdown_line(f\"**Response:**\\n\\n{model_response_content}\")\n",
    "                    )\n",
    "                    md_file.write(\n",
    "                        format_markdown_line(\n",
    "                            f\"**Retrieved Documents:**\\n\\n{retrieved_docs_content}\"\n",
    "                        )\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Error appending to markdown file: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Step 8: Update previous response content for the next question\n",
    "            previous_response_content = model_response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing questions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5056/3406119728.py:19: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "json_path = 'questions.json'\n",
    "output_md_file = 'questions_results.md'\n",
    "process_questions_from_json(json_path, output_md_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
