{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/workspaces/AI_Chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.51GB\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from constants import CHROMA_SETTINGS, PERSIST_DIRECTORY, SOURCE_DIRECTORY\n",
    "from callback_logger import CallbackLogger\n",
    "import performance_logger \n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from semantic_chunking_helper import SematicChunkingHelper\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger('__file__')\n",
    "\n",
    "source_dir = SOURCE_DIRECTORY\n",
    "\n",
    "model_name='hkunlp/instructor-xl'\n",
    "persist_dir=PERSIST_DIRECTORY + \"_\" + model_name\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/AI_Chatbot/selfcheck/.DB_hkunlp/instructor-xl\n"
     ]
    }
   ],
   "source": [
    "print(persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-10-01 18:11:09,315 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-xl\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": device},\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=persist_dir,\n",
    "    embedding_function=embeddings,\n",
    "    client_settings=CHROMA_SETTINGS,\n",
    ")\n",
    "\n",
    "\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "model = HuggingFaceCrossEncoder(\n",
    "    model_name=\"BAAI/bge-reranker-base\", model_kwargs={\"device\": device}\n",
    ")\n",
    "reranker = CrossEncoderReranker(model=model, top_n=2)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_chain(model: ChatOllama):\n",
    "    \"\"\"Create chat history\"\"\"\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "                                        which might reference context in the chat history, formulate a standalone question \\\n",
    "                                        which can be understood without the chat history. Do NOT answer the question, \\\n",
    "                                        just reformulate it if needed and otherwise return it as is.\n",
    "                                    \"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        model, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"\n",
    "                        You are a helpful DEK assistant for question-answering DEK policies. \\\n",
    "                        Do not give me any information outside of PROVIDED CONTEXT. \\\n",
    "                        If you don't know the answer, just say that you don't know. \\\n",
    "                        You have to answer the question in Vietnamese. \\\n",
    "                        {context}\n",
    "                        \"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | qa_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        history_aware_retriever, question_answer_chain\n",
    "    ).assign(answer=chain_from_docs)\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain_community.cross_encoders.huggingface.HuggingFaceCrossEncoder'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_conversational_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mcreate_conversational_chain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      3\u001b[0m contextualize_q_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven a chat history and the latest user question \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m                                    which might reference context in the chat history, formulate a standalone question \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m                                    which can be understood without the chat history. Do NOT answer the question, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m                                    just reformulate it if needed and otherwise return it as is.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m                                \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m contextualize_q_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m      9\u001b[0m     [\n\u001b[1;32m     10\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, contextualize_q_system_prompt),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m history_aware_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_history_aware_retriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontextualize_q_prompt\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m qa_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m                    You are a helpful DEK assistant for question-answering DEK policies. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m                    Do not give me any information outside of PROVIDED CONTEXT. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m                    \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m                    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     27\u001b[0m qa_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     28\u001b[0m     [\n\u001b[1;32m     29\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, qa_system_prompt),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     ]\n\u001b[1;32m     33\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/history_aware_retriever.py:65\u001b[0m, in \u001b[0;36mcreate_history_aware_retriever\u001b[0;34m(llm, retriever, prompt)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `input` to be a prompt variable, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     57\u001b[0m retrieve_documents: RetrieverOutputLike \u001b[38;5;241m=\u001b[39m RunnableBranch(\n\u001b[1;32m     58\u001b[0m     (\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# Both empty string and empty list evaluate to False\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# If no chat history, then we just pass input to retriever\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         (\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m|\u001b[39m retriever,\n\u001b[1;32m     63\u001b[0m     ),\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# If chat history, then we pass inputs to LLM chain, then to retriever\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m \u001b[38;5;241m|\u001b[39m StrOutputParser() \u001b[38;5;241m|\u001b[39m retriever,\n\u001b[1;32m     66\u001b[0m )\u001b[38;5;241m.\u001b[39mwith_config(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_retriever_chain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retrieve_documents\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py:421\u001b[0m, in \u001b[0;36mRunnable.__or__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__or__\u001b[39m(\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    413\u001b[0m     other: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     ],\n\u001b[1;32m    419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RunnableSerializable[Input, Other]:\n\u001b[1;32m    420\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\u001b[38;5;28mself\u001b[39m, \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py:4977\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   4975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[1;32m   4976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4978\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4979\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4980\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'langchain_community.cross_encoders.huggingface.HuggingFaceCrossEncoder'>"
     ]
    }
   ],
   "source": [
    "chain = create_conversational_chain(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_answer_from_response(response: dict):\n",
    "    \"\"\"filter_answer_from_response\"\"\"\n",
    "    if response[\"answer\"] or len(response[\"answer\"]) > 0:\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"</s> [INST]\", \"\")\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"</s>\", \"\")\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"<s>\", \"\")\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"[ANSW]\", \"\")\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"[ANS]\", \"\")\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"[/ANSW]\", \"\")\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"[INST]\", \"\")\n",
    "        response[\"answer\"] = response[\"answer\"].replace(\"[/INST]\", \"\")\n",
    "\n",
    "def ask(query: str, chat_history: list):\n",
    "    \"\"\"Retrieve answer from LLM\"\"\"\n",
    "    if not chain:\n",
    "        return \"Please, add a document first.\"\n",
    "\n",
    "    performance_logger.open(query, model=model_name)\n",
    "    callback_handler = CallbackLogger(logger=performance_logger)\n",
    "    config = {\"callbacks\": [callback_handler]}\n",
    "\n",
    "    result = chain.invoke(\n",
    "        {\"input\": query, \"chat_history\": chat_history}, config=config\n",
    "    )\n",
    "    performance_logger.close(result)\n",
    "\n",
    "    filter_answer_from_response(result)\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
